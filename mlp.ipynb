{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wFCIy8HvRpun"
      },
      "id": "wFCIy8HvRpun"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Userfrom1995/CreditCard_Predictions/raw/main/validation_data_to_be_shared.zip"
      ],
      "metadata": {
        "id": "vn65gL8PRMjv",
        "outputId": "13903433-bf32-45cc-f3a1-da0056ae608a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "vn65gL8PRMjv",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-15 10:50:31--  https://github.com/Userfrom1995/CreditCard_Predictions/raw/main/validation_data_to_be_shared.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Userfrom1995/CreditCard_Predictions/main/validation_data_to_be_shared.zip [following]\n",
            "--2025-01-15 10:50:31--  https://raw.githubusercontent.com/Userfrom1995/CreditCard_Predictions/main/validation_data_to_be_shared.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22086713 (21M) [application/zip]\n",
            "Saving to: ‘validation_data_to_be_shared.zip’\n",
            "\n",
            "validation_data_to_ 100%[===================>]  21.06M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-01-15 10:50:32 (150 MB/s) - ‘validation_data_to_be_shared.zip’ saved [22086713/22086713]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Userfrom1995/CreditCard_Predictions/raw/main/Dev_data_to_be_shared.zip"
      ],
      "metadata": {
        "id": "q-VfHisCRMaf",
        "outputId": "358be472-c8eb-419d-c591-cbdf90d23bda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "q-VfHisCRMaf",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-15 10:50:33--  https://github.com/Userfrom1995/CreditCard_Predictions/raw/main/Dev_data_to_be_shared.zip\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Userfrom1995/CreditCard_Predictions/main/Dev_data_to_be_shared.zip [following]\n",
            "--2025-01-15 10:50:33--  https://raw.githubusercontent.com/Userfrom1995/CreditCard_Predictions/main/Dev_data_to_be_shared.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 51165748 (49M) [application/zip]\n",
            "Saving to: ‘Dev_data_to_be_shared.zip’\n",
            "\n",
            "Dev_data_to_be_shar 100%[===================>]  48.79M   212MB/s    in 0.2s    \n",
            "\n",
            "2025-01-15 10:50:34 (212 MB/s) - ‘Dev_data_to_be_shared.zip’ saved [51165748/51165748]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Dev_data_to_be_shared.zip\n",
        "!unzip validation_data_to_be_shared.zip"
      ],
      "metadata": {
        "id": "73smKHMvRMPE",
        "outputId": "323a20e5-20e3-4c2f-e835-6b768410b21c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "73smKHMvRMPE",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Dev_data_to_be_shared.zip\n",
            "  inflating: Dev_data_to_be_shared.csv  \n",
            "Archive:  validation_data_to_be_shared.zip\n",
            "  inflating: validation_data_to_be_shared.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Check number of CPU cores\n",
        "cpu_cores = os.cpu_count()\n",
        "print(f\"Number of CPU cores available: {cpu_cores}\")\n"
      ],
      "metadata": {
        "id": "4eyhYg23RL-u",
        "outputId": "1cf55a1c-b997-498e-b89c-583b61e3c427",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4eyhYg23RL-u",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of CPU cores available: 96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install py-cpuinfo\n"
      ],
      "metadata": {
        "id": "G0proBaLSLT1",
        "outputId": "d4832670-8cc6-4e2e-9224-62909663a6eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "G0proBaLSLT1",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting py-cpuinfo\n",
            "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
            "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: py-cpuinfo\n",
            "Successfully installed py-cpuinfo-9.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import platform\n",
        "\n",
        "# Get processor information\n",
        "cpu_info = platform.processor()\n",
        "print(f\"Processor: {cpu_info}\")\n"
      ],
      "metadata": {
        "id": "5E9NtHySSOa1",
        "outputId": "ebc30dc5-e8a6-409d-dce1-cea04b41f193",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5E9NtHySSOa1",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processor: x86_64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cpuinfo\n",
        "\n",
        "# Get CPU information\n",
        "cpu_info = cpuinfo.get_cpu_info()\n",
        "print(f\"CPU Brand: {cpu_info['brand_raw']}\")\n",
        "print(f\"Architecture: {cpu_info['arch']}\")\n"
      ],
      "metadata": {
        "id": "nZAD8f1qSRyP",
        "outputId": "4a2b722a-4ab4-4b4a-b797-6a917c4ffb2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nZAD8f1qSRyP",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU Brand: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "Architecture: X86_64\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T09:07:04.842555Z",
          "start_time": "2025-01-15T09:06:54.074730Z"
        },
        "id": "c825a6d9934a696c",
        "outputId": "457d8007-e7c2-480e-9968-fd37207059c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# from imblearn.combine import SMOTETomek\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "# Replace 'your_dataset.csv' with the actual file path\n",
        "data = pd.read_csv(\"Dev_data_to_be_shared.csv\")\n",
        "\n",
        "# Step 2: Separate features (X) and target variable (y)\n",
        "# Replace 'bad_flag' with the name of your target column\n",
        "X = data.drop(columns=[\"bad_flag\",\"account_number\"])  # Features\n",
        "y = data[\"bad_flag\"]\n",
        "X.fillna(0, inplace=True)# Target variable\n",
        "\n",
        "# Step 3: Train-Test Split\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Apply SMOTE-Tomek to balance the classes in the training set\n",
        "# smt = SMOTETomek(random_state=42)\n",
        "# X_train_resampled, y_train_resampled = smt.fit_resample(X_train, y_train)\n",
        "\n",
        "# Step 5: Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_original = scaler.fit_transform(X_train)  # Scale the resampled training data\n",
        "X_test_original = scaler.transform(X_test)\n",
        "X_original = scaler.transform(X)# Scale the test data\n",
        "\n",
        "# Print data shapes for verification\n",
        "print(f\"Original Training Set Shape (X_train): {X_train.shape}, (y_train): {y_train.shape}\")\n",
        "# print(f\"Resampled Training Set Shape (X_train_resampled): {X_train_resampled.shape}, (y_train_resampled): {y_train.shape}\")\n",
        "print(f\"Test Set Shape (X_test): {X_test.shape}, (y_test): {y_test.shape}\")\n",
        "\n"
      ],
      "id": "c825a6d9934a696c",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Training Set Shape (X_train): (77444, 1214), (y_train): (77444,)\n",
            "Test Set Shape (X_test): (19362, 1214), (y_test): (19362,)\n"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-15T09:07:08.698325Z",
          "start_time": "2025-01-15T09:07:08.107192Z"
        },
        "id": "c726facb1dd8d1f",
        "outputId": "7fe8d06e-e45e-4f19-aa8b-0f532323b2c4"
      },
      "cell_type": "code",
      "source": [
        "print(X_resampled_scaled.shape)\n",
        "print(y_resampled.shape)"
      ],
      "id": "c726facb1dd8d1f",
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X_resampled_scaled' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mX_resampled_scaled\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_resampled\u001b[38;5;241m.\u001b[39mshape)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'X_resampled_scaled' is not defined"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn\n"
      ],
      "metadata": {
        "id": "SHRNFsKRS6iq",
        "outputId": "7e10c24c-f620-41c2-caa4-6c612f3e1ef8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "SHRNFsKRS6iq",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting imbalanced-learn\n",
            "  Downloading imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.6.0)\n",
            "Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn)\n",
            "  Downloading sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "Downloading imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/238.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m235.5/238.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.4/238.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: sklearn-compat, imbalanced-learn\n",
            "Successfully installed imbalanced-learn-0.13.0 sklearn-compat-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "is_executing": true
        },
        "ExecuteTime": {
          "start_time": "2025-01-15T08:52:39.288184Z"
        },
        "id": "initial_id",
        "outputId": "f27f233f-0f25-4045-f0c5-70cef1a20b57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "source": [
        "import joblib  # Library for saving and loading models\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Resample using SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# # Convert the dataset to float32 to reduce memory usage\n",
        "# X_resampled = X_resampled.astype(np.float32)\n",
        "# X_test = X_test.astype(np.float32)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(128,), (256,), (512,), (256, 128), (512, 256)],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'solver': ['adam', 'sgd'],\n",
        "    'alpha': [0.0001, 0.001, 0.01],\n",
        "    'learning_rate': ['constant', 'adaptive'],\n",
        "    'max_iter': [10],\n",
        "    'early_stopping': [True]\n",
        "}\n",
        "\n",
        "# Initialize MLPClassifier\n",
        "mlp = MLPClassifier(random_state=42)\n",
        "\n",
        "# Perform Grid Search\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=mlp,\n",
        "    param_grid=param_grid,\n",
        "    scoring='f1',\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit Grid Search\n",
        "grid_search.fit(X_resampled_scaled, y_resampled)\n",
        "\n",
        "# Save all results\n",
        "results = pd.DataFrame(grid_search.cv_results_)\n",
        "results.to_csv('mlp_hyperparameter_tuning_results.csv', index=False)\n",
        "\n",
        "# Print total models tested and top 5 models\n",
        "print(f\"Total models tested: {len(results)}\")\n",
        "print(\"Top 5 models:\")\n",
        "print(results[['mean_test_score', 'std_test_score', 'params']].sort_values(by='mean_test_score', ascending=False).head())\n",
        "\n",
        "# Save the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "joblib.dump(best_model, \"best_mlp_model.joblib\")\n",
        "print(\"Best model saved as 'best_mlp_model.joblib'\")\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-2802b1a4b78e>\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Fit Grid Search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_resampled_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_resampled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Save all results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1021\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    967\u001b[0m                     )\n\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    970\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    971\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n"
      ],
      "metadata": {
        "id": "xdSXkyAYYCrx",
        "outputId": "0d54f5a8-4e2e-437b-cd8c-174d84657ab5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "xdSXkyAYYCrx",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Resample using SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(128,), (256,), (512,), (256, 128), (512, 256)],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'solver': ['adam', 'sgd'],\n",
        "    'alpha': [0.0001, 0.001, 0.01],\n",
        "    'learning_rate': ['constant', 'adaptive'],\n",
        "    'max_iter': [10],\n",
        "    'early_stopping': [True]\n",
        "}\n",
        "\n",
        "# Initialize MLPClassifier\n",
        "mlp = MLPClassifier(random_state=42)\n",
        "\n",
        "# Wrap GridSearchCV with tqdm\n",
        "class TQDMGridSearchCV(GridSearchCV):\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        # Total number of fits\n",
        "        n_candidates = len(self.param_grid['hidden_layer_sizes']) * len(self.param_grid['activation']) * \\\n",
        "                       len(self.param_grid['solver']) * len(self.param_grid['alpha']) * \\\n",
        "                       len(self.param_grid['learning_rate'])\n",
        "        n_splits = self.cv\n",
        "        total_fits = n_candidates * n_splits\n",
        "\n",
        "        with tqdm(total=total_fits, desc=\"Grid Search Progress\") as pbar:\n",
        "            # Add a custom callback to update tqdm progress\n",
        "            self._tqdm = pbar\n",
        "            return super().fit(X, y, **fit_params)\n",
        "\n",
        "    def _fit_and_score(self, *args, **kwargs):\n",
        "        result = super()._fit_and_score(*args, **kwargs)\n",
        "        self._tqdm.update(1)  # Update progress bar for each fit\n",
        "        return result\n",
        "\n",
        "# Perform Grid Search with TQDMGridSearchCV\n",
        "grid_search = TQDMGridSearchCV(\n",
        "    estimator=mlp,\n",
        "    param_grid=param_grid,\n",
        "    scoring='f1',\n",
        "    cv=3,\n",
        "    verbose=0,  # Turn off sklearn verbosity since tqdm handles it\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_resampled_scaled, y_resampled)\n",
        "\n",
        "# Save all results\n",
        "results = pd.DataFrame(grid_search.cv_results_)\n",
        "results.to_csv('mlp_hyperparameter_tuning_results.csv', index=False)\n",
        "\n",
        "# Print total models tested and top 5 models\n",
        "print(f\"Total models tested: {len(results)}\")\n",
        "print(\"Top 5 models:\")\n",
        "print(results[['mean_test_score', 'std_test_score', 'params']].sort_values(by='mean_test_score', ascending=False).head())\n",
        "\n",
        "# Save the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "joblib.dump(best_model, \"best_mlp_model.joblib\")\n",
        "print(\"Best model saved as 'best_mlp_model.joblib'\")\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "FPKDZ5BTYKey",
        "outputId": "8e04351c-3e5e-4b07-f696-1180af1fed02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "FPKDZ5BTYKey",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Grid Search Progress:   0%|          | 0/360 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "Grid Search Progress:   0%|          | 0/360 [01:50<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total models tested: 120\n",
            "Top 5 models:\n",
            "    mean_test_score  std_test_score  \\\n",
            "34         0.988614        0.004396   \n",
            "32         0.988614        0.004396   \n",
            "54         0.988614        0.004396   \n",
            "52         0.988614        0.004396   \n",
            "12         0.988614        0.004396   \n",
            "\n",
            "                                               params  \n",
            "34  {'activation': 'relu', 'alpha': 0.001, 'early_...  \n",
            "32  {'activation': 'relu', 'alpha': 0.001, 'early_...  \n",
            "54  {'activation': 'relu', 'alpha': 0.01, 'early_s...  \n",
            "52  {'activation': 'relu', 'alpha': 0.01, 'early_s...  \n",
            "12  {'activation': 'relu', 'alpha': 0.0001, 'early...  \n",
            "Best model saved as 'best_mlp_model.joblib'\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99     94481\n",
            "           1       0.04      0.02      0.02      1357\n",
            "\n",
            "    accuracy                           0.98     95838\n",
            "   macro avg       0.51      0.51      0.51     95838\n",
            "weighted avg       0.97      0.98      0.98     95838\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "# Resample using SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(128,), (256,), (512,), (256, 128), (512, 256)],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'solver': ['adam', 'sgd'],\n",
        "    'alpha': [0.0001, 0.001, 0.01],\n",
        "    'learning_rate': ['constant', 'adaptive'],\n",
        "    'max_iter': [10],\n",
        "    'early_stopping': [True]\n",
        "}\n",
        "\n",
        "# Initialize MLPClassifier\n",
        "mlp = MLPClassifier(random_state=42, verbose=True)  # Enable verbose output\n",
        "\n",
        "# Custom GridSearchCV to include tqdm and iteration tracking\n",
        "class TQDMGridSearchCV(GridSearchCV):\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        n_candidates = len(self.param_grid['hidden_layer_sizes']) * len(self.param_grid['activation']) * \\\n",
        "                       len(self.param_grid['solver']) * len(self.param_grid['alpha']) * \\\n",
        "                       len(self.param_grid['learning_rate'])\n",
        "        n_splits = self.cv\n",
        "        total_fits = n_candidates * n_splits\n",
        "\n",
        "        with tqdm(total=total_fits, desc=\"Grid Search Progress\") as pbar:\n",
        "            self._tqdm = pbar\n",
        "            self._current_fit = 0\n",
        "\n",
        "            # Capture MLPClassifier's verbose output to track iterations\n",
        "            old_stdout = sys.stdout\n",
        "            sys.stdout = open('/dev/null', 'w')  # Suppress stdout temporarily\n",
        "\n",
        "            try:\n",
        "                # Wrap training to capture iteration progress\n",
        "                return super().fit(X, y, **fit_params)\n",
        "            finally:\n",
        "                sys.stdout = old_stdout\n",
        "\n",
        "    def _fit_and_score(self, *args, **kwargs):\n",
        "        # Log the current fit index\n",
        "        self._current_fit += 1\n",
        "        print(f\"Starting fit {self._current_fit}/{self._tqdm.total}...\")\n",
        "\n",
        "        # Enable iteration tracking per fit\n",
        "        model = args[0]\n",
        "        if hasattr(model, 'verbose'):\n",
        "            model.verbose = True\n",
        "\n",
        "        # Perform the fit and track the progress\n",
        "        result = super()._fit_and_score(*args, **kwargs)\n",
        "        self._tqdm.update(1)\n",
        "        return result\n",
        "\n",
        "# Perform Grid Search\n",
        "grid_search = TQDMGridSearchCV(\n",
        "    estimator=mlp,\n",
        "    param_grid=param_grid,\n",
        "    scoring='f1',\n",
        "    cv=3,\n",
        "    verbose=0,  # Disable default GridSearchCV verbose\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_resampled_scaled, y_resampled)\n",
        "\n",
        "# Save all results\n",
        "results = pd.DataFrame(grid_search.cv_results_)\n",
        "results.to_csv('mlp_hyperparameter_tuning_results.csv', index=False)\n",
        "\n",
        "# Print total models tested and top 5 models\n",
        "print(f\"Total models tested: {len(results)}\")\n",
        "print(\"Top 5 models:\")\n",
        "print(results[['mean_test_score', 'std_test_score', 'params']].sort_values(by='mean_test_score', ascending=False).head())\n",
        "\n",
        "# Save the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "joblib.dump(best_model, \"best_mlp_model.joblib\")\n",
        "print(\"Best model saved as 'best_mlp_model.joblib'\")\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "cKGYDMbZZlK3",
        "outputId": "9dea8085-58f3-441d-df80-9f254c339944",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cKGYDMbZZlK3",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Grid Search Progress:   0%|          | 0/360 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "Grid Search Progress:   0%|          | 0/360 [01:53<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total models tested: 120\n",
            "Top 5 models:\n",
            "    mean_test_score  std_test_score  \\\n",
            "34         0.988614        0.004396   \n",
            "32         0.988614        0.004396   \n",
            "54         0.988614        0.004396   \n",
            "52         0.988614        0.004396   \n",
            "12         0.988614        0.004396   \n",
            "\n",
            "                                               params  \n",
            "34  {'activation': 'relu', 'alpha': 0.001, 'early_...  \n",
            "32  {'activation': 'relu', 'alpha': 0.001, 'early_...  \n",
            "54  {'activation': 'relu', 'alpha': 0.01, 'early_s...  \n",
            "52  {'activation': 'relu', 'alpha': 0.01, 'early_s...  \n",
            "12  {'activation': 'relu', 'alpha': 0.0001, 'early...  \n",
            "Best model saved as 'best_mlp_model.joblib'\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99     94481\n",
            "           1       0.04      0.02      0.02      1357\n",
            "\n",
            "    accuracy                           0.98     95838\n",
            "   macro avg       0.51      0.51      0.51     95838\n",
            "weighted avg       0.97      0.98      0.98     95838\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Resample using SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(128,), (256,), (512,), (256, 128), (512, 256)],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'solver': ['adam', 'sgd'],\n",
        "    'alpha': [0.0001, 0.001, 0.01],\n",
        "    'learning_rate': ['constant', 'adaptive'],\n",
        "    'max_iter': [10],\n",
        "    'early_stopping': [True]\n",
        "}\n",
        "\n",
        "# Initialize progress bar for overall progress\n",
        "total_fits = len(param_grid['hidden_layer_sizes']) * len(param_grid['activation']) * \\\n",
        "             len(param_grid['solver']) * len(param_grid['alpha']) * len(param_grid['learning_rate']) * 3  # cv=3\n",
        "progress_bar = tqdm(total=total_fits, desc=\"Grid Search Progress\")\n",
        "\n",
        "\n",
        "# Custom Wrapper for MLPClassifier to Log Iterations\n",
        "class IterationTrackingMLPClassifier(MLPClassifier):\n",
        "    def _fit(self, X, y, incremental=False):\n",
        "        \"\"\"Override the _fit method to track iterations.\"\"\"\n",
        "        n_iter = 0\n",
        "        max_iter = self.max_iter\n",
        "\n",
        "        while n_iter < max_iter:\n",
        "            super()._fit(X, y, incremental=incremental)\n",
        "            n_iter += 1\n",
        "            print(f\"   Iteration {n_iter}/{max_iter} completed.\")\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "# Initialize MLPClassifier with Iteration Tracking\n",
        "mlp = IterationTrackingMLPClassifier(random_state=42, verbose=True)\n",
        "\n",
        "# Custom GridSearchCV to Track Progress\n",
        "class TQDMGridSearchCV(GridSearchCV):\n",
        "    def _fit_and_score(self, estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score=False):\n",
        "        # Update tqdm progress for each fit\n",
        "        progress_bar.update(1)\n",
        "        print(f\"Starting fit {progress_bar.n}/{progress_bar.total}...\")\n",
        "        return super()._fit_and_score(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score)\n",
        "\n",
        "\n",
        "# Perform Grid Search\n",
        "grid_search = TQDMGridSearchCV(\n",
        "    estimator=mlp,\n",
        "    param_grid=param_grid,\n",
        "    scoring='f1',\n",
        "    cv=3,\n",
        "    verbose=0,  # Disable sklearn verbosity\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_resampled_scaled, y_resampled)\n",
        "\n",
        "# Save all results\n",
        "results = pd.DataFrame(grid_search.cv_results_)\n",
        "results.to_csv('mlp_hyperparameter_tuning_results.csv', index=False)\n",
        "\n",
        "# Print total models tested and top 5 models\n",
        "print(f\"Total models tested: {len(results)}\")\n",
        "print(\"Top 5 models:\")\n",
        "print(results[['mean_test_score', 'std_test_score', 'params']].sort_values(by='mean_test_score', ascending=False).head())\n",
        "\n",
        "# Save the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "joblib.dump(best_model, \"best_mlp_model.joblib\")\n",
        "print(\"Best model saved as 'best_mlp_model.joblib'\")\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Close progress bar\n",
        "progress_bar.close()\n"
      ],
      "metadata": {
        "id": "3OnZToyvbcDT"
      },
      "id": "3OnZToyvbcDT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import joblib\n",
        "\n",
        "# Resample using SMOTE\n",
        "print(\"Applying SMOTE to handle class imbalance...\")\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Standardize the features\n",
        "print(\"Standardizing the features...\")\n",
        "scaler = StandardScaler()\n",
        "X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the MLP model\n",
        "print(\"Defining the MLP model...\")\n",
        "mlp_model = MLPClassifier(\n",
        "    hidden_layer_sizes=(512, 256),  # Two hidden layers\n",
        "    activation=\"relu\",             # ReLU activation function\n",
        "    solver=\"adam\",                 # Adam optimizer\n",
        "    alpha=0.0001,                  # L2 regularization term\n",
        "    max_iter=50,                   # Number of iterations\n",
        "    early_stopping=True,           # Stop early if validation score doesn't improve\n",
        "    random_state=42,               # Random state for reproducibility\n",
        "    verbose=True                   # Display training progress\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"Training the MLP model...\")\n",
        "mlp_model.fit(X_resampled_scaled, y_resampled)\n",
        "print(f\"Number of iterations completed: {mlp_model.n_iter_}\")\n",
        "\n",
        "# Save the trained model\n",
        "model_path = \"new_mlp_model_with_smote.joblib\"\n",
        "joblib.dump(mlp_model, model_path)\n",
        "print(f\"New MLP model saved to '{model_path}'\")\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "print(\"Evaluating the model on the test dataset...\")\n",
        "y_pred = mlp_model.predict(X_test_scaled)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Optional: Save the predictions to a CSV file\n",
        "import pandas as pd\n",
        "predictions = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred})\n",
        "predictions.to_csv(\"new_mlp_model_predictions.csv\", index=False)\n",
        "print(\"Predictions saved to 'new_mlp_model_predictions.csv'\")\n"
      ],
      "metadata": {
        "id": "TcIHkjfFbpNS",
        "outputId": "63f04c7b-a2cd-40ad-ab87-ab04b4edcd0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "TcIHkjfFbpNS",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying SMOTE to handle class imbalance...\n",
            "Standardizing the features...\n",
            "Defining the MLP model...\n",
            "Training the MLP model...\n",
            "Iteration 1, loss = 0.14745739\n",
            "Validation score: 0.966337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iterations completed: 1\n",
            "New MLP model saved to 'new_mlp_model_with_smote.joblib'\n",
            "Evaluating the model on the test dataset...\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.95      0.97     19092\n",
            "           1       0.04      0.15      0.06       270\n",
            "\n",
            "    accuracy                           0.94     19362\n",
            "   macro avg       0.51      0.55      0.51     19362\n",
            "weighted avg       0.97      0.94      0.95     19362\n",
            "\n",
            "Predictions saved to 'new_mlp_model_predictions.csv'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}